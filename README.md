# Асинхронный веб-скрапер и сервер с кэшированием

## Описание проекта

Этот проект включает в себя две основные части: клиентский скрипт и сервер, работающий с использованием FastAPI. Клиентский скрипт (`crawl.py`) отправляет асинхронные запросы на сервер для обработки URL-адресов и получения их статуса. Сервер (`server_cached.py`) принимает эти запросы, асинхронно обрабатывает URL-адреса, кэширует результаты и возвращает статус выполнения задачи.

## Используемые инструменты

- **Python**: Весь проект написан на языке Python с использованием асинхронной библиотеки asyncio.
- **FastAPI**: FastAPI используется для создания асинхронного веб-сервера для обработки запросов.
- **aiohttp**: Библиотека aiohttp используется для асинхронной работы с HTTP-запросами.
- **Redis**: Redis используется для кэширования статуса запросов и управления задачами.
- **uvicorn**: Сервер запускается с помощью uvicorn для обработки запросов FastAPI.

## Настройка и запуск проекта

1. Установите необходимые зависимости, запустив `pip install -r requirements.txt`.
2. Запустите сервер, выполните `python server_cached.py`.
3. Для запуска клиентского скрипта укажите URL-адреса в качестве аргументов командной строки: `python crawl.py http://example.com`.
4. Наблюдайте вывод статусов обработки URL-адресов.

## Кэширование и управление задачами

- Каждая задача имеет уникальный идентификатор и может находиться в состояниях "running" или "ready".
- Результаты обработки кэшируются на уровне URL-адресов для избежания повторной обработки.

